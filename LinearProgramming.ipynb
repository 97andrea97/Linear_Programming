{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Programming (LP)\n",
    "LP is an optimization technique for a linear objective function, subject to linear equality and linear inequality constraints. Linear functions are always convex. \n",
    "Its feasible region (set of points which satisfy all the constraints) is a convex polytope described by the constraint inequalities. So we're interested in finding the point belonging to the convex polytope (look definitions at the end of this notebook) in which the function has the minimum value.\n",
    "\n",
    "\n",
    "# typical problems:\n",
    "LP is typically used for problems which would require to look for all possible permutations of assignation of values to the decision variables. For instance suppose you have K1 kg of corns, K2 kg of malt and you must decide how much of them deploy to produce beer and how much to deploy whisky to have the maximum profit.\n",
    "\n",
    "# CANONICAL FORM:\n",
    "\n",
    "Sometimes called standard form, but standard form is a term used also to refer to the augmented form.\n",
    "WHEN IS REQUIRED THE CANONICAL FORM YOU MUST TRANSFORM YOUR PROBLEM. ANY PROBLEM, SUCH AS A MINIMIZATION PROBLEM, PROBLEM WITH CONSTRAINTS IN ALTERNATIVE FORM, WITH NEGATIVE VARIABLES ETC CAN ALWAYS BE REWRITTEN INTO AN EQUIVALENT PROBLEM IN CANONICAL FORM.\n",
    "\n",
    "The canonical form has three parts:\n",
    "1) A linear function to be maximized. \n",
    "2) Problem constraints.\n",
    "3) Non-negative variable\n",
    "\n",
    "WHICH MUST BE EXPRESSED IN THIS FORM:\n",
    "$${\\displaystyle {\\begin{aligned}&{\\text{Maximize}}&&\\mathbf {c} ^{\\mathrm {T} }\\mathbf {x} \\\\&{\\text{subject to}}&&A\\mathbf {x} \\leq \\mathbf {b} \\\\&{\\text{and}}&&\\mathbf {x} \\geq \\mathbf {0} \\end{aligned}}}$$\n",
    "\n",
    "\n",
    "also expressed as: $\\quad{\\displaystyle \\max\\{\\mathbf {c} ^{\\mathrm {T} }\\mathbf {x} \\;|\\;A\\mathbf {x} \\leq \\mathbf {b} \\land \\mathbf {x} \\geq 0\\}}$\n",
    "\n",
    "\n",
    "where x represents the vector of variables (to be determined), c and b are vectors of (known) coefficients, A is a (known) matrix of coefficients. The inequalities Ax ≤ b and x ≥ 0 are the constraints which specify the feasible region (a convex polytope).\n",
    "\n",
    "Or can also be expressed as a \"tableau\":\n",
    "\n",
    "\n",
    "$${\\displaystyle {\\begin{bmatrix}1&-\\mathbf {c} ^{T}&0\\\\0&\\mathbf {A} &\\mathbf {b} \\end{bmatrix}}}$$\n",
    "\n",
    "\n",
    "\n",
    "Example:\n",
    "\n",
    "1) $ {\\displaystyle f(x_{1},x_{2})=c_{1}x_{1}+c_{2}x_{2}} $\n",
    " \n",
    "2) ${\\displaystyle {\\begin{matrix}a_{11}x_{1}+a_{12}x_{2}&\\leq b_{1}\\\\a_{21}x_{1}+a_{22}x_{2}&\\leq b_{2}\\\\a_{31}x_{1}+a_{32}x_{2}&\\leq b_{3}\\\\\\end{matrix}}}$\n",
    "  \n",
    "3) ${\\displaystyle {\\begin{matrix}x_{1}\\geq 0\\\\x_{2}\\geq 0\\end{matrix}}}$\n",
    "\n",
    "# STANDARD/AUGMENTED/SLACK FORM:\n",
    "It's the original problem to which are added some variables in order to transform inequalities in equalities. Each slack variable is like taking account of how much the point is far from the correspondent edge of the feasible region:\n",
    "\n",
    "<img src=\"slack.png\" width=50% height=50%>\n",
    "(in the example the slack variables are $x_3,x_4,x_5$)\n",
    "\n",
    "\n",
    "Many problems can be efficiently solved by using the \"simplex method\". This requires to convert the problem in the augmented form. We discussed the standard/canonical form before because it is also the base for the augmented one.\n",
    "\n",
    "The augmented form introduces non-negative slack variables to replace inequalities with equalities in the constraints. The slack variables are called usually $\\vec{s}$, or sometimes is used the same notation for the decision variables, but it's indicated strarting from which value of pedice they are to be considered slack variables. In matrix form the augmented form is:\n",
    "\n",
    "Maximize ${\\displaystyle z}:$\n",
    "$${\\displaystyle {\\begin{bmatrix}1&-\\mathbf {c} ^{T}& \\mathbf {0} ^{T}\\\\\\mathbf{0}&\\mathbf {A} &\\mathbf {I} \\end{bmatrix}}{\\begin{bmatrix}z\\\\\\mathbf {x} \\\\\\mathbf {s} \\end{bmatrix}}={\\begin{bmatrix}0\\\\\\mathbf {b} \\end{bmatrix}}}$$\n",
    "\n",
    "$${\\displaystyle \\mathbf {x} \\geq 0,\\mathbf {s} \\geq 0}$$\n",
    "\n",
    "where ${\\displaystyle \\mathbf {s} }$  are the newly introduced slack variables, ${\\displaystyle \\mathbf {x} }$  are the decision variables, and ${\\displaystyle z}$ is the variable to be maximized.\n",
    "\n",
    "\n",
    "Example: \n",
    "\n",
    "consider this problem in standard form:\n",
    "\n",
    "Maximze: $ {\\displaystyle f(x_{1},x_{2})=c_{1}x_{1}+c_{2}x_{2}} $\n",
    " \n",
    " ${\\displaystyle {\\begin{matrix}a_{11}x_{1}+a_{12}x_{2}&\\leq b_{1}\\\\a_{21}x_{1}+a_{22}x_{2}&\\leq b_{2}\\\\\\end{matrix}}}$\n",
    "  \n",
    " ${\\displaystyle {\\begin{matrix}x_{1}\\geq 0\\\\x_{2}\\geq 0\\end{matrix}}}$\n",
    " \n",
    "Conversion into augmented form (non matrix form): (simply add a slack variable and change the inequality in equality, and add the constraint of being >= 0 also for the slack variables)\n",
    "\n",
    "Maximze: $ {\\displaystyle f(x_{1},x_{2})=c_{1}x_{1}+c_{2}x_{2}} $\n",
    " \n",
    " ${\\displaystyle {\\begin{matrix}a_{11}x_{1}+a_{12}x_{2} + x_3 & = b_{1}\\\\a_{21}x_{1}+a_{22}x_{2} + x_4 & = b_{2}\\\\\\end{matrix}}}$\n",
    "  \n",
    " ${\\displaystyle {\\begin{matrix}x_{1}\\geq 0\\\\x_{2}\\geq 0\\\\x_{3}\\geq 0\\\\x_{4}\\geq 0\\end{matrix}}}$\n",
    "\n",
    "Where $x_3$ and $x_4$ are the slack variables (which could also be called $s_1$ and $s_2$).\n",
    " \n",
    "COnversion of it into the augmented form in matrix notation: (note that now there's an additional variable z, and the problem becomes how to maimize z!!!)\n",
    "\n",
    "Maximize ${\\displaystyle z}$:\n",
    "\n",
    "$${\\displaystyle {\\begin{bmatrix}1 & - c_1& -c_2 & 0 & 0 \\\\ 0& a_{11} & a_{12} & 1 &0 \\\\ 0 & a_{21} & a{22} & 0 & 1  \\end{bmatrix}}\n",
    "{\\begin{bmatrix}z\\\\ x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4  \\end{bmatrix}}= {\\begin{bmatrix}0 \\\\ b_1 \\\\ b_2 \\end{bmatrix}}}$$\n",
    "\n",
    "$${\\displaystyle \\mathbf {x} \\geq 0,\\mathbf {s} \\geq 0}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duality\n",
    "This is a concept used not nly in LP. But in LP in particular the meaning is this:\n",
    "Every LP problem is also called \"primal problem\"; it can be converted into a \"dual problem\", which provides an upper bound to the optimal value of the primal problem. \n",
    "\n",
    "It's something based on the farkas lemma, read about it at the end of the notebook if you wnat. The duality can be showe as a consequence of applying the Lagrangian method to the linear problem. Indeed remember that the Lagrangian function \"L\" can be introduced to delete the inequality and equality constraints. Then remember that you can find the extrema of the objective fucntion by looking for the seadle points of the L, because you look for the points in which L has a minimum wrt the decision variables and a maximum wrt the lagrangian multipliers. The primal problem is like looking for the points in which L is minimum wrt the decision variables, among the points in which L is maximum wrt lagrangian multipliers. Instead the dual problem flips it: look for the points in which the L is maximum wrt the lagrangian multipliers among the points in which L is minimum wrt the decision variables.\n",
    "\n",
    "There are two kinds of duality:\n",
    "\n",
    "- symmetric duality:\n",
    "  \n",
    "  the (\"primal\") Lp problem:\n",
    "  \n",
    "  \"Maximize $\\quad{\\displaystyle \\{\\mathbf {c} ^{\\mathrm {T} }\\mathbf {x} \\;|\\;A\\mathbf {x} \\leq \\mathbf {b} \\land \\mathbf {x} \\geq 0\\}}$\" \n",
    "  \n",
    "  can be converted in the symmetric dual problem:\n",
    "  \n",
    "  \" MINIMIZE $\\quad{\\displaystyle \\{\\mathbf {b} ^{\\mathrm {T} }\\mathbf {y} \\;|\\;A^\\mathrm {T} \\mathbf {y} \\leq \\mathbf {c} \\land \\mathbf {y} \\geq 0\\}}$ \"\n",
    "  \n",
    "  Property of the symmetric dality:  the symm. dual of a symm. dual linear program is the original primal linear program.\n",
    "  \n",
    "  \n",
    "  \n",
    "\n",
    "- asymmetric duality:\n",
    "\n",
    "   the (\"primal\") Lp problem:\n",
    "   \n",
    "   \"Maximize $\\quad{\\displaystyle \\{\\mathbf {c} ^{\\mathrm {T} }\\mathbf {x} \\;|\\;A\\mathbf {x} \\leq \\mathbf {b} \\}}$\"  (note that there are not the constraints about x being non-negative)\n",
    "  \n",
    "   can be converted in the symmetric dual problem:\n",
    "   \n",
    "   \" MINIMIZE $\\quad{\\displaystyle \\{\\mathbf {b} ^{\\mathrm {T} }\\mathbf {y} \\;|\\;A^\\mathrm {T} \\mathbf {y} = \\mathbf {c} \\land \\mathbf {y} \\geq 0\\}}$ \" (note that the inequality constraints become equality, and note that the non-negative constraints about y are still to be considered)\n",
    "  \n",
    "  \n",
    "PROPERTIES about LP (but not integer programming):\n",
    "\n",
    "- The weak duality theorem states that the objective function value of the dual at any feasible solution is always $\\geq$ to the objective function value of the primal at any feasible solution. (the dual solution is an upper bound of the primal)\n",
    "\n",
    "-  The strong duality theorem states that if the primal has an optimal solution, x*, then the dual also has an optimal solution, y*, and this holds: $c^Tx*=b^Ty*$.\n",
    "\n",
    "- complementary slackness theorem...\n",
    "\n",
    "Why is duality important:\n",
    "- to determine the optimality of a solution \n",
    "- to determine the sensitivity of a solution to (small) changes of the problem parameters. The dual variable on a constraint represents the incremental change in the optimal solution value per unit increase in the RHS of the constraint. \n",
    "- Identifying near-optimal solutions: a good dual solution can be used to bound the values of primal solutions, and it can be used to identify when a primal solution is nearoptimal. \n",
    "- Karush-Kuhn-Tucker conditions: the optimal solution to the dual problem is a vector of KKT multipliers. \n",
    "- Convergence of improvement algorithms: the dual problem can be used in the convergence analysis of algorithms. \n",
    "- Good structure: often, the dual problem has some good mathematical, geometric, or computational structure that can exploited in computing solutions to both the primal and the dual problem.\n",
    "\n",
    "example of duality:\n",
    "In models of electrical networks the current flows are “primal variables\" and the voltage differences are the “dual variables\" that arise in consideration of optimization (and equilibrium) in electrical networks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LP Properties:\n",
    "A Linear function is always convex(concave), so both the local minimum(maximum) also global. The optimal solution always exist but in two cases: the problem is infeasible (the feasible region is null), or the feasible region is unbounded (the optimal solution could be at infinite).\n",
    "\n",
    "WHERE CAN WE LOOK FOR THE OPTIMAL SOLUTION?\n",
    "- EXTREME POINT: . A vector $\\mathbf{x}$ of a polytode P is called an extreme point if there are no two vectors $\\mathbf{y} \\neq  \\mathbf{x}$ and $\\mathbf{z} \\neq  \\mathbf{x}$ in P such that x is a linear combination of y and z. This means an extreme point is a vector which does not lie on the line connecting any two vectors in P. A vertex is also called basic feasible solution (BFS).\n",
    "\n",
    "\n",
    "- EXTREME POINT THEOREM: If there exists an optimal solution to standard form LP, then there exists one that is an extreme point. \n",
    "\n",
    "\n",
    "HOW TO FIND THE OPTIMAL SOLUTION?\n",
    "\n",
    "In general is ok to have a greedy algorithm since if the optimal solution exist, a local extrema is also a global one.\n",
    "\n",
    "There are two classes of solution methods: \n",
    "\n",
    "1) Simplex algorithms move on the surface of the polytope \n",
    "\n",
    "2) Interior-point algorithms move within the polytope\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simplex algorithms:\n",
    "It is a family of algorithms of which main idea is to begin at a starting vertex and move along the edges of the polytope until it's reached the vertex of the optimal solution.\n",
    "\n",
    "<img src = \"simplex_algo.png\" width=30% height=30%>\n",
    "\n",
    "The simplex algorithms work on the standard/canonical form.\n",
    "It is based on two properties (of which we don't study the proof):\n",
    "\n",
    "- If the objective function has a maximum value on the feasible region, then it has this value on (at least) one of the extreme points.\n",
    "\n",
    "- If an extreme point is not a maximum point of the objective function, then there is an edge containing the considered point so that the objective function is strictly increasing on the edge moving away from the point\n",
    "\n",
    "To find a solution the problem is approached in two steps:\n",
    "1) Phase 1: a starting extreme point is found. It can be done by applying the simplex algorithm to a modified version of the original program. The possible results of Phase I are either that a basic feasible solution is found or that the feasible region is empty.\n",
    "\n",
    "2) Phase 2: The simplex algorithm is applied using the basic feasible solution found in Phase I as a starting point. The possible results from Phase II are either an optimum basic feasible solution or an infinite edge on which the objective function is unbounded above.\n",
    "\n",
    "Simplex algorithm steps:\n",
    "- 1.Find a corner of the feasible region\n",
    "- 2.Repeat:\n",
    "    - 2.1 For each of the n hyperplanes intersecting at the corner, calculate its reduced cost \n",
    "    - 2.2 If they are all positive (or 0), then STOP (because that's an optimal solution)\n",
    "           else, pick the most negative reduced cost\n",
    "    - 2.3 Move along corresponding edge until the next corner is reached \n",
    "\n",
    "(where the reduced cost for a hyperplane at a vertex is the change of the objective function of moving one unit away from the plane along its corresponding edge. To each edges which are link to the considered vertex i must compute the reduce cost, and use it to decide in which edge move). FOR MINIMIZATION PROBLEMS I MOVE WHERE THE REDUCE COST IS MINIMUM (BECAUSE IT IMPLIES THE HIGHEST DECREASE OF THE OBJ FUNC), BUT THIS IS ONLY AN HEURISTIC. The pseudocode written indeed is about the family of simplex algorithms, the specific ones changes the heuristic and/or the data structure used.\n",
    "\n",
    "COMPUTATIONAL COST:\n",
    "\n",
    "The worst case complexity is exponential, but in very rare cases, usually it is polinomial.\n",
    "\n",
    "We'll see many implementation of this algorithm in the course of algorithms. Now just let's see a possible implementation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t10\t90\t40\t50\t400\t900\t0\t0\t0\t0\t\n",
      "\n",
      "0\t\n",
      "-110\t\n",
      "-205\t\n",
      "-160\t\n",
      "-160\t\n",
      "-420\t\n",
      "-260\t\n",
      "1\t\n",
      "0\t\n",
      "0\t\n",
      "-2e+03\t\n",
      "0\t\n",
      "-4\t\n",
      "-32\t\n",
      "-13\t\n",
      "-8\t\n",
      "-4\t\n",
      "-14\t\n",
      "0\t\n",
      "1\t\n",
      "0\t\n",
      "-55\t\n",
      "0\t\n",
      "-2\t\n",
      "-12\t\n",
      "-54\t\n",
      "-285\t\n",
      "-22\t\n",
      "-80\t\n",
      "0\t\n",
      "0\t\n",
      "1\t\n",
      "-800\t\n",
      "Obj val= [  1.  10.  90.  40.  50. 400. 900.   0.   0.   0.   0.]\n"
     ]
    }
   ],
   "source": [
    "from numpy import *\n",
    "import pandas as pd\n",
    "import os\n",
    " \n",
    "class Tableau:\n",
    " \n",
    "    def __init__(self, obj):\n",
    "        self.obj = [1] + obj\n",
    "        self.rows = []\n",
    "        self.cons = []\n",
    " \n",
    "    def add_constraint(self, expression, value):\n",
    "        self.rows.append([0] + expression)\n",
    "        self.cons.append(value)\n",
    " \n",
    "    def _pivot_column(self):\n",
    "        low = 0\n",
    "        idx = 0\n",
    "        for i in range(1, len(self.obj)-1):\n",
    "            if self.obj[i] < low:\n",
    "                low = self.obj[i]\n",
    "                idx = i\n",
    "        if idx == 0: return -1\n",
    "        return idx\n",
    " \n",
    "    def _pivot_row(self, col):\n",
    "        rhs = [self.rows[i][-1] for i in range(len(self.rows))]\n",
    "        lhs = [self.rows[i][col] for i in range(len(self.rows))]\n",
    "        ratio = []\n",
    "        for i in range(len(rhs)):\n",
    "            if lhs[i] == 0:\n",
    "                ratio.append(99999999 * abs(max(rhs)))\n",
    "                continue\n",
    "            ratio.append(rhs[i]/lhs[i])\n",
    "        return argmin(ratio)\n",
    " \n",
    "    def display(self):\n",
    "        for j in range(0,len(self.obj)):\n",
    "            print(\"{0:.3g}\".format(self.obj[j]), end=\"\\t\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "        for i in range(0,len(t.rows)):\n",
    "            for j in range(0,len(self.obj)):\n",
    "                print(\"{0:.3g}\".format(t.rows[i][j]), end=\"\\t\")\n",
    "                print()\n",
    " \n",
    "    def _pivot(self, row, col):\n",
    "        e = self.rows[row][col]\n",
    "        self.rows[row] /= e\n",
    "        for r in range(len(self.rows)):\n",
    "            if r == row: continue\n",
    "            self.rows[r] = self.rows[r] - self.rows[r][col]*self.rows[row]\n",
    "        self.obj = self.obj - self.obj[col]*self.rows[row]\n",
    " \n",
    "    def _check(self):\n",
    "        if min(self.obj[1:-1]) >= 0: return 1\n",
    "        return 0\n",
    "     \n",
    "    def solve(self):\n",
    " \n",
    "        # build full tableau\n",
    "        for i in range(len(self.rows)):\n",
    "            self.obj += [0]\n",
    "            ident = [0 for r in range(len(self.rows))]\n",
    "            ident[i] = 1\n",
    "            self.rows[i] += ident + [self.cons[i]]\n",
    "            self.rows[i] = array(self.rows[i], dtype=float)\n",
    "        self.obj = array(self.obj + [0], dtype=float)\n",
    " \n",
    "        # solve\n",
    "        self.display()\n",
    "        while not self._check():\n",
    "            c = self._pivot_column()\n",
    "            r = self._pivot_row(c)\n",
    "            self._pivot(r,c)\n",
    "            print ('\\n pivot column: %s\\n pivot row: %s'%(c+1,r+2) )\n",
    "            self.display()\n",
    "\n",
    "        print(\"Obj val= {}\".format(self.obj))\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    #os.chdir('/AAAToBackup/didattica/Math programming/Optimization @ AI/code')\n",
    "    #df = pd.read_csv('data/brewery.csv', sep=',',header=None)\n",
    "    df = pd.read_csv('dieta1.csv', sep=',',header=None)\n",
    "    table = df.values\n",
    "    m,n = table.shape\n",
    "\n",
    "    numvar = n-1\n",
    "    numcon = m-1\n",
    "\n",
    "    t = Tableau(list(table[0][0:numvar]))\n",
    "    for i in range(1,numcon+1):\n",
    "        t.add_constraint(list(table[i][0:numvar]),table[i][-1])\n",
    "\n",
    "    t.solve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simplex problem using python, with pupl library:\n",
    "It uses the MIP algorithms, which are the most powerful algorithms for this topic, with this line: pulp.PULP_CBC_CMD(fracGap = 0.00001, maxSeconds = 500, threads = None)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Optimal\n",
      "Total revenue =  800.0\n",
      "Ale = 12.0\n",
      "Beer = 28.0\n",
      "Corn_availability : 5*Ale + 15*Beer <= 480 \t dual -1.0 \tslack -0.0\n",
      "Hops_availability : 4*Ale + 4*Beer <= 160 \t dual -2.0 \tslack -0.0\n",
      "Malt_availability : 35*Ale + 20*Beer <= 1190 \t dual 0.0 \tslack 210.0\n"
     ]
    }
   ],
   "source": [
    "import pulp\n",
    "\n",
    "\n",
    "\n",
    "# prob contains the problem data, problem defined as min\n",
    "prob = pulp.LpProblem(\"The Brewery Problem\", pulp.LpMinimize)\n",
    "\n",
    " \n",
    "\n",
    "# The 2 variables Beer and Alre are created with a lower limit of zero\n",
    "x1=pulp.LpVariable(\"Beer\", 0) # cat=\"Integer\"\n",
    "x2=pulp.LpVariable(\"Ale\", 0)\n",
    "\n",
    " \n",
    "\n",
    "# The objective function is min, profits are negative\n",
    "prob += -23*x1 -13*x2, \"Total profit per unit of product\"\n",
    "\n",
    " \n",
    "\n",
    "# Availability constraints\n",
    "prob += 15*x1 + 5*x2 <= 480, \"Corn availability\"\n",
    "prob += 4*x1 + 4*x2 <= 160, \"Hops availability\"\n",
    "prob += 20*x1 + 35*x2 <= 1190, \"Malt availability\"\n",
    "\n",
    " \n",
    "\n",
    "# The problem data is written to an .lp file\n",
    "prob.writeLP(\"Brewery.lp\")\n",
    "\n",
    " \n",
    "\n",
    "# The problem is solved using PuLP's choice of Solver or\n",
    "#prob.solve() # let pulp decide the solver\n",
    "#prob.solve(CPLEX())\n",
    "prob.solve(pulp.PULP_CBC_CMD(fracGap = 0.00001, maxSeconds = 500, threads = None))\n",
    "\n",
    " \n",
    "\n",
    "# The status of the solution\n",
    "print(\"Status:\", pulp.LpStatus[prob.status])\n",
    "\n",
    " \n",
    "\n",
    "# The optimal objective function value\n",
    "print(\"Total revenue = \", -1*pulp.value(prob.objective))\n",
    "\n",
    " \n",
    "\n",
    "# Primal and dual variables optimal value\n",
    "for v in prob.variables():\n",
    "    print(v.name, \"=\", v.varValue)\n",
    "\n",
    " \n",
    "\n",
    "for name, c in list(prob.constraints.items()):\n",
    "    print(name, \":\", c, \"\\t dual\", c.pi, \"\\tslack\", c.slack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some definitions:\n",
    "\n",
    "- \"POLYTOPE\" = It is a generalization in n-dimension of the objects like \"polygon\"(in 2D) and \"polyhedron\"(in 3D), which main property is that it's made of \"flat\" sides. \n",
    "\n",
    "\n",
    "- \"CONVEX POLYTOPE\" = It's the intersection of a set of half-spaces. (So it can be neither bounded nor finite).\n",
    "\n",
    "\n",
    "- \"HALF-SPACES\" = It's either of the two parts into which a hyperplane divides an affine space. (for example considering the 3D space, the hyperplane can be any plane, and the half spaces are the two portions of the 3D space on the two sides of the plane. If the space is 2D the half-spaces are called half-planes). It is said \"open half-space\" if the hyperplane is not considered part of the half space. An half-space can be described by a linear inequality:$$ {\\displaystyle a_{1}x_{1}+a_{2}x_{2}+\\cdots +a_{n}x_{n}>b} $$\n",
    "\n",
    "  - Using only $>$ the inequality describes an \"open\" half-space. Using $\\geq$ it's a \"closed\" half-space.\n",
    "  - if $ x_n \\geq 0 $ and the inequality uses $>$ or $\\geq\", then the half-space is called \"upper\".\n",
    "  - Property: an half space is a convex set.\n",
    "  \n",
    "  \n",
    "-  \"AFFINE SPACE\" = informal definition: an affine space is nothing more than a vector space whose origin we try to forget about, by adding translations to the linear maps...\n",
    "\n",
    "\n",
    "- \"CONVEX SET\" = A set is convex if considering any two points of it, it contains the whole line segment that joins them.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# farkas lemma\n",
    "It is the base of duality for LP, but also used to proof the KKT conditions in NLP.\n",
    "Lemma:\n",
    "\n",
    "Let ${\\displaystyle \\mathbf {A} \\in \\mathbb {R} ^{m\\times n}}$ and  ${\\displaystyle \\mathbf {b} \\in \\mathbb {R} ^{m}}$ . Then exactly one of the following two statements is true:\n",
    "\n",
    "- There exists an ${\\displaystyle \\mathbf {x} \\in \\mathbb {R} ^{n}}$ such that ${\\displaystyle \\mathbf {Ax} =\\mathbf {b} }$  and ${\\displaystyle \\mathbf {x} \\geq 0}$.\n",
    "\n",
    "\n",
    "- There exists a ${\\displaystyle \\mathbf {y} \\in \\mathbb {R} ^{m}}$ such that ${\\displaystyle \\mathbf {A} ^{\\mathsf {T}}\\mathbf {y} \\geq 0}$ and ${\\displaystyle \\mathbf {b} ^{\\mathsf {T}}\\mathbf {y} <0}$.\n",
    "\n",
    "One of the use of this is that if there's no solution you should be able to check it. In particular in LP this is used to check if the Point which is considered can be improved: it can't be improved if the objective function computed in that point falls into the cone described by the columns of a, so I know when a point is the optimal solution thanks to this lemma!\n",
    "\n",
    "Geometric interpretation of the farkas lemma: https://demonstrations.wolfram.com/FarkassLemmaInTwoDimensions/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
